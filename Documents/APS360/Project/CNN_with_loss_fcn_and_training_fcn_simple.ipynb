{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def load_dataset(base_dir):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for genre in os.listdir(base_dir):\n",
        "        genre_dir = os.path.join(base_dir, genre)\n",
        "        if not os.path.isdir(genre_dir):\n",
        "            continue\n",
        "        for fname in os.listdir(genre_dir):\n",
        "            if fname.endswith('.pt'):\n",
        "                path = os.path.join(genre_dir, fname)\n",
        "                tensor = torch.load(path).flatten().numpy()\n",
        "                features.append(tensor)\n",
        "                labels.append(genre)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "file_dir = \"/content/drive/MyDrive/split_data_small (1)/\"\n",
        "\n",
        "X_train, y_train = load_dataset(file_dir + \"train\")\n",
        "X_val, y_val = load_dataset(file_dir + \"val\")\n",
        "X_test, y_test = load_dataset(file_dir + \"test\")\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_val = le.transform(y_val)\n",
        "y_test = le.transform(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT-s6Y1FLBVk",
        "outputId": "0fd257c9-c212-4dbd-85f8-b2e882bc0146"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\n",
        "\n",
        "def get_data_loader(\n",
        "    target_classes,\n",
        "    batch_size=64,\n",
        "    root_dir=\"/content/drive/MyDrive/split_data_small (1)/\",\n",
        "    random_seed=1000\n",
        "):\n",
        "    \"\"\"\n",
        "    Adapted for music genre classification using pre-processed .pt files\n",
        "\n",
        "    Args:\n",
        "        target_classes: List of genre classes to include (or None for all)\n",
        "        batch_size: samples per batch\n",
        "        root_dir: path to directory containing train/val/test folders\n",
        "        random_seed: for reproducible results\n",
        "\n",
        "    Returns:\n",
        "        train_loader, val_loader, test_loader, classes\n",
        "    \"\"\"\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # Load all datasets using existing function\n",
        "    X_train, y_train = load_dataset(root_dir + \"train\")\n",
        "    X_val, y_val = load_dataset(root_dir + \"val\")\n",
        "    X_test, y_test = load_dataset(root_dir + \"test\")\n",
        "\n",
        "    # Encode labels using existing label encoder\n",
        "    y_train_encoded = le.fit_transform(y_train)\n",
        "    y_val_encoded = le.transform(y_val)\n",
        "    y_test_encoded = le.transform(y_test)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.FloatTensor(X_train)\n",
        "    y_train_tensor = torch.LongTensor(y_train_encoded)\n",
        "    X_val_tensor = torch.FloatTensor(X_val)\n",
        "    y_val_tensor = torch.LongTensor(y_val_encoded)\n",
        "    X_test_tensor = torch.FloatTensor(X_test)\n",
        "    y_test_tensor = torch.LongTensor(y_test_encoded)\n",
        "\n",
        "    # Reshape to 224x224 images (inputs are confirmed to be this size)\n",
        "    X_train_tensor = X_train_tensor.view(-1, 1, 224, 224)\n",
        "    X_val_tensor = X_val_tensor.view(-1, 1, 224, 224)\n",
        "    X_test_tensor = X_test_tensor.view(-1, 1, 224, 224)\n",
        "\n",
        "    # Create TensorDatasets\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Get class names from label encoder\n",
        "    classes = le.classes_.tolist()\n",
        "\n",
        "    # Filter by target_classes if specified\n",
        "    if target_classes is not None:\n",
        "        # This would require more complex filtering logic\n",
        "        # For now, we'll just return all classes\n",
        "        pass\n",
        "\n",
        "    print(f\"Loaded {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test samples\")\n",
        "    print(f\"Classes: {classes}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, classes"
      ],
      "metadata": {
        "id": "q1fMR_UTQiJo"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_multiclass(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_err = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            total_err += (preds != labels).sum().item()\n",
        "            total_loss += loss.item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    avg_err = total_err / total_samples\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_err, avg_loss\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_per_class(model, dataloader, le, criterion):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    report = classification_report(all_labels, all_preds, target_names=le.classes_)\n",
        "    print(f\"\\nTest Set Classification Report (Avg Loss: {avg_loss:.4f}):\\n\")\n",
        "    print(report)\n",
        "\n",
        "def get_model_name(name, batch_size, learning_rate, epoch, base_dir=\"models\"):\n",
        "    # Create base directory if it doesn't exist\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "    # Format model path\n",
        "    path = os.path.join(base_dir, \"model_{0}_bs{1}_lr{2}_epoch{3}.pt\".format(\n",
        "        name, batch_size, learning_rate, epoch))\n",
        "\n",
        "    return path"
      ],
      "metadata": {
        "id": "5at7rmNCQW7a"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "Dv-lDL8dBpGT"
      },
      "outputs": [],
      "source": [
        "#stuff for model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# normalize data (penalizing weights so to speak)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# activation function\n",
        "import numpy as np\n",
        "from scipy.special import erf\n",
        "\n",
        "#(dont use this for now; guarantee values w/ reLU first)\n",
        "def activation(x):\n",
        "  return -x * erf(np.exp(-x))\n",
        "\n",
        "# pooling\n",
        "def pool(dim, kernel, stride = 1, padding = 0):\n",
        "  out = ((dim + 2*padding - kernel) // stride ) + 1\n",
        "  return out\n",
        "\n",
        "# model\n",
        "class GlizzyNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(GlizzyNet, self).__init__()\n",
        "        self.name = \"GlizzyNet\"\n",
        "\n",
        "        # Sequential js blocks everythign tgether so its a little easier to read\n",
        "        self.blockOne = nn.Sequential(\n",
        "            nn.Conv2d(1, 5, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "        )\n",
        "\n",
        "        self.blockTwo = nn.Sequential(\n",
        "            nn.Conv2d(5, 10, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "        )\n",
        "\n",
        "        self.blockThree = nn.Sequential(\n",
        "            nn.Conv2d(10, 15, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "        )\n",
        "\n",
        "        # find the output size; uh js change input_dim (ruian)\n",
        "        dim = 224\n",
        "        for i in range (3):\n",
        "          # conv(kernel=5, stride=0, pad=0)\n",
        "          dim = pool(dim, kernel=5)  # conv layer\n",
        "          dim = pool(dim, kernel=2, stride=2)  # pool layer\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(15 * dim * dim, 32) # 32 hidden neurons like in lab 2\n",
        "        # now connect 32 to 9 (instead of 32 to 1 in lab 2)\n",
        "        self.fc2 = nn.Linear(32, num_classes)\n",
        "\n",
        "    # the forward pass function\n",
        "    def forward(self, x):\n",
        "        x = self.blockOne(x)\n",
        "        x = self.blockTwo(x)\n",
        "        x = self.blockThree(x)\n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "#training function\n",
        "def train_net(net, batch_size=64, learning_rate=0.01, num_epochs=30, target_classes=None):\n",
        "    ########################################################################\n",
        "    # Train a classifier on music genres\n",
        "    if target_classes is None:\n",
        "        raise ValueError(\"target_classes must be specified for music genre classification\")\n",
        "\n",
        "    torch.manual_seed(1000)\n",
        "\n",
        "    ########################################################################\n",
        "    # Obtain the PyTorch data loader objects to load batches of the datasets\n",
        "    train_loader, val_loader, test_loader, classes = get_data_loader(target_classes, batch_size)\n",
        "\n",
        "    ########################################################################\n",
        "    # Define the Loss function and optimizer\n",
        "    # Compute weights\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights_tensor = torch.FloatTensor(class_weights)\n",
        "\n",
        "    # Use weighted loss\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(net.parameters(), lr = learning_rate) # idk this is js a placeholder value for the lr im not acc too sure which is good\n",
        "\n",
        "    ########################################################################\n",
        "    # Set up arrays to store training/validation metrics\n",
        "    train_err = np.zeros(num_epochs)\n",
        "    train_loss = np.zeros(num_epochs)\n",
        "    val_err = np.zeros(num_epochs)\n",
        "    val_loss = np.zeros(num_epochs)\n",
        "\n",
        "    ########################################################################\n",
        "    # Train the network\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        total_train_loss = 0.0\n",
        "        total_train_err = 0.0\n",
        "        total_samples = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)  # shape: [batch_size, num_classes]\n",
        "            loss = criterion(outputs, labels)  # labels are class indices [0, ..., C-1]\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute number of incorrect predictions\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            total_train_err += (preds != labels).sum().item()\n",
        "            total_train_loss += loss.item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        train_err[epoch] = total_train_err / total_samples\n",
        "        train_loss[epoch] = total_train_loss / (i + 1)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_err[epoch], val_loss[epoch] = evaluate_multiclass(net, val_loader, criterion)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: \"\n",
        "              f\"Train err: {train_err[epoch]:.4f}, Train loss: {train_loss[epoch]:.4f} | \"\n",
        "              f\"Val err: {val_err[epoch]:.4f}, Val loss: {val_loss[epoch]:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
        "        torch.save(net.state_dict(), model_path)\n",
        "\n",
        "    print('Finished Training')\n",
        "    print(f\"Total time elapsed: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Save logs for plotting\n",
        "    np.savetxt(f\"{model_path}_train_err.csv\", train_err)\n",
        "    np.savetxt(f\"{model_path}_train_loss.csv\", train_loss)\n",
        "    np.savetxt(f\"{model_path}_val_err.csv\", val_err)\n",
        "    np.savetxt(f\"{model_path}_val_loss.csv\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = GlizzyNet(num_classes=8)\n",
        "batch_size = 64\n",
        "learning_rate = 0.0005\n",
        "num_epochs = 30  # Start with fewer epochs for testing\n",
        "target_classes = le.classes_.tolist()  # Use all classes\n",
        "train_net(net, batch_size=batch_size, learning_rate=learning_rate, num_epochs=num_epochs, target_classes=target_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vqs75ugLo_T",
        "outputId": "af0d8ee0-f866-4676-a2ec-d049643fedcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 470 training, 101 validation, 101 test samples\n",
            "Classes: ['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International', 'Pop', 'Rock']\n",
            "Epoch 1: Train err: 0.8809, Train loss: 3.8881 | Val err: 0.8911, Val loss: 2.0822\n",
            "Epoch 2: Train err: 0.8638, Train loss: 2.0833 | Val err: 0.8911, Val loss: 2.0833\n",
            "Epoch 3: Train err: 0.8957, Train loss: 2.0908 | Val err: 0.8911, Val loss: 2.0796\n",
            "Epoch 4: Train err: 0.8915, Train loss: 2.0860 | Val err: 0.8911, Val loss: 2.0843\n",
            "Epoch 5: Train err: 0.8894, Train loss: 2.0821 | Val err: 0.8911, Val loss: 2.0748\n",
            "Epoch 6: Train err: 0.8574, Train loss: 2.0652 | Val err: 0.8317, Val loss: 2.0562\n",
            "Epoch 7: Train err: 0.8213, Train loss: 2.0512 | Val err: 0.8911, Val loss: 2.1091\n",
            "Epoch 8: Train err: 0.8745, Train loss: 2.0396 | Val err: 0.8416, Val loss: 2.0593\n",
            "Epoch 9: Train err: 0.7426, Train loss: 2.0231 | Val err: 0.8515, Val loss: 2.0616\n",
            "Epoch 10: Train err: 0.7936, Train loss: 1.9919 | Val err: 0.8416, Val loss: 2.1039\n",
            "Epoch 11: Train err: 0.8255, Train loss: 2.0671 | Val err: 0.8020, Val loss: 2.0593\n",
            "Epoch 12: Train err: 0.8468, Train loss: 2.0675 | Val err: 0.8614, Val loss: 2.0837\n",
            "Epoch 13: Train err: 0.8596, Train loss: 2.0760 | Val err: 0.8614, Val loss: 2.0764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Curve (borrowed from lab 2)\n",
        "def plot_training_curve(path):\n",
        "    \"\"\" Plots the training curve for a model run, given the csv files\n",
        "    containing the train/validation error/loss.\n",
        "\n",
        "    Args:\n",
        "        path: The base path of the csv files produced during training\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
        "    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
        "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
        "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
        "    plt.title(\"Train vs Validation Error\")\n",
        "    n = len(train_err) # number of epochs\n",
        "    plt.plot(range(1,n+1), train_err, label=\"Train\")\n",
        "    plt.plot(range(1,n+1), val_err, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Error\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    plt.title(\"Train vs Validation Loss\")\n",
        "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
        "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "final_model_path = get_model_name('GlizzyNet', 64, 0.0005, 29)  # adjust epoch number\n",
        "plot_training_curve(final_model_path)"
      ],
      "metadata": {
        "id": "iUXUr7GQfk1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final test on testing data after training model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "_, _, test_loader, _ = get_data_loader(target_classes, batch_size)\n",
        "evaluate_per_class(net, test_loader, le, criterion)"
      ],
      "metadata": {
        "id": "Mdn6uj2xR44c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}